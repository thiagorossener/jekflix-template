---
date: 2022-03-24 12:26:40
layout: post
title: "Just Rank: Rethinking Evaluation with Word and Sentence Similarities"
subtitle:
description: hello
image: /assets/img/posts/Just-Rank/just_rank_img.png
optimized_image: /assets/img/posts/Just-Rank/just_rank_img.png
category: Deep Learning
tags:
  - Deep Learning Paper
  - nlp-sentence-embedding
author: dayone
paginate: true
comment: true
---


{%katexmm%}
- [Paper Link](https://arxiv.org/pdf/2203.02679v2.pdf)
- [Takeaways](#takeaways)
- [Approach](#approach)
    - [ğŸ’¡ Theory and Motivation](#-theory-and-motivation)
    - [EvalRank](#evalrank)
    - [Methodology](#methodology)
    - [Datasets Collection](#datasets-collection)
    - [Good Intrinsic Evaluator](#good-intrinsic-evaluator)
- [Result](#result)
    - [Sentence Level Experiments](#sentence-level-experiments)
    - [Sentence Level Result](#sentence-level-result)
- [Background Knowledge](#background-knowledge)
    - [ê¸°ì¡´ embedding êµ¬í•˜ëŠ” ë°©ë²•](#ê¸°ì¡´-embedding-êµ¬í•˜ëŠ”-ë°©ë²•)
    - [ë¬¸ì œì ](#ë¬¸ì œì )


# Takeaways

---

- retrievalë“±ì˜ downstream taskì— ì“°ì´ëŠ” embeddingì„ ë½‘ê¸° ìœ„í•œ encoderí•™ìŠµì— stsì˜ ëŒ€ì•ˆì¸ `EvalRank`ë¥¼ ì œì‹œí•¨. ì‹¤í—˜í•´ë³¼ ê°€ì¹˜ê°€ ìˆë‹¤ê³  ìƒê°.
- spreading-activation theoryì—ì„œ ì˜ê°ì„ ë°›ìŒ
- extensive experiments with 60+ models and 10 downstream tasks

# Approach

---

### ğŸ’¡ Theory and Motivation
 - Spreading-Activation Theory: explains how concepts store and interact within the human brain
 the concept network
   - only highly related concepts are connected
   -To find the relatedness between concepts like engine and street, the activation is spreading through mediating concepts like car and ambulance with decaying factors.
 - To test the soundness of the concept network, it is enough to ensure the local connectivity between concepts
 - the long-distance relationships can be inferred with various spreading activation algorithms


![Untitled](/assets/img/posts/Just-Rank/Untitled.png)

### EvalRank

- test only on highly related pairs and make sure they are topologically close in the embedding spac
- ê¸°ìˆ ëœ ë¬¸ì œì  ì™„í™”
    1. focus on highly related pairs, which are intuitive to human annotators
    2. stronger correlation with downstream tasks as desired properties are measured
    3. less affected by the whitening methods (treat the embedding space from a local perspective)

### Methodology

- frame the evaluation of embeddings as a retrieval task
- EvalRank dataset
    1. {% katex %} P = \{p_1, p_2 . . . p_m\} {% endkatex %}
    2. {% katex %}C =  \{c_1, c_2, . . . c_n\}{% endkatex %}
    - Each positive pair {% katex %}p_i{% endkatex %}  in {% katex %}P{% endkatex %} consists of two samples in {% katex %}C{% endkatex %} that are semantically similar:
    {% katex %}p_i = (c_x, c_y){% endkatex %}
- For each sample $c_x$ and its positive correspondence {% katex %}c_y{% endkatex %} a good embedding model should has their embeddings {% katex %}(e_x, e_y){% endkatex %} close in the embedding space.
    - the other background smaples should locate farther away from the sameple $c_x$
- the similarity score is used to sort all background samples in descending order
    - the performance at each positive pair {% katex %}p_i{% endkatex %} is measured by the rank of $c_x$ â€˜s positive correspondence $c_y$ w.r.t all background samples
    - $rank_i = rank(S(c_x, c_y), [||_{j=1, j \ne x}^n s(c_x, c_j)])$
        - $||$ : concatenate operation
    - To measure the overall performance MRR and Hits@k scores are used
        - $MRR = \frac{1}{m} \Sigma_{i=1}^m \frac{1}{rank_i}$
        - $Hits@k = \frac{1}{m} \Sigma_{i=1}^k 1[rank_i \leq k]$

### Datasets Collection

- STS-Benchmark, STR datasetì—ì„œ ìƒìœ„ 25% ìœ ì‚¬ë„ë¥¼ ë³´ì´ëŠ” pairë“¤ì´ positive pairë¡œ ì“°ì„
- reversed pairë„ positive pairë¡œ ì“°ì„

### Good Intrinsic Evaluator

- EvalRankê°€ ë” ì¢‹ì€ embeddingì˜ indicatorì„ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´, word/sentence embeddingì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” intrinsic evaluator (sts, EvalRank)ì™€ downstream taskì—ì„œì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•œ í›„ ê·¸ ìƒê´€ê´€ê³„ë¥¼ ì¸¡ì •í•¨
- EvalRankê°€ ë” ì¢‹ì€ ì§€í‘œì´ê¸° ìœ„í•´ì„œëŠ” EvalRankì™€ downstream tasksì˜ ì„±ëŠ¥ì´ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì—¬ì•¼ í•œë‹¤.

# Result

---

### Sentence Level Experiments

- Setup
    - 67 embedding models

### Sentence Level Result

- Downstream task performanceì™€ì˜ ìƒê´€ê´€ê³„

    ![Untitled](/assets/img/posts/Just-Rank/Untitled%201.png)

    - EvalRank outperforms all sts datasets with a clear margin
- Visualization

    ![Untitled](/assets/img/posts/Just-Rank/Untitled%202.png)

    - Model performance on MR is pivot
        - MR taskì—ì„œ ì˜í•˜ëŠ” ìˆœì„œëŒ€ë¡œ rankingì„ í–ˆìŒ
        - SST ëŠ” ìœ ì‚¬í•œ sentiment analysisë¼ì„œ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì„
        - STS-Bë‚˜ STRì— ë¹„í•´ì„œ íŒŒë€ìƒ‰ EvalRankê°€ ë” ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì¸ë‹¤
        -

# Background Knowledge

---

### ê¸°ì¡´ embedding êµ¬í•˜ëŠ” ë°©ë²•

1. word/sentence embeddingì„ êµ¬í•œë‹¤ (ì£¼ë¡œ language model)
2. word/sentence pairì˜ similarityë¥¼ êµ¬í•œë‹¤
    - Cosine similarityê°€ ëŒ€ë¶€ë¶„ì˜ caseì—ì„œ ì“°ì„
3. similarity scoreë¥¼ human annotationì˜ gtì™€ correlationì„ êµ¬í•œë‹¤

### ë¬¸ì œì 

- sentence similarityê¸°ë°˜ì˜ embeddingì„ êµ¬í•˜ëŠ” ë°©ë²•ì€ downstream taskì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŒ
    1. the definition of similarity is too vague (multifaceted relationships)
        1. the concept of similarity and relatedness are not well-defined
            1. ìœ ì‚¬í•œ ë¬¸ì¥ì€ ì—°ê´€ë˜ì–´ìˆì§€ë§Œ ì—°ê´€ë˜ì—ˆë‹¤ê³  ìœ ì‚¬í•˜ì§€ëŠ” ì•Šë‹¤.
            2. relationships between samples are far more complicated than currently considered, which is challenge to all current datasets
        2. the annotation process is not intuitive to humans
            1. the instructions on similarity levels are not well defined
            2. priming effect theory: ì‚¬ëŒì€ ë¹„êµë¥¼ ì˜í•˜ê¸° ìœ„í•´ì„œëŠ” pivot sampleì´ í•„ìš”í•˜ë‹¤.
                1. it is more intutitive for human to compare $(a, b) > (a,c)$ than $(a,b) > (c, d)$ as far as similarity is concerned
                2. ì½”ë¼ë¦¬ì™€ íŒ½ê·„ì˜ ìœ ì‚¬ë„ vs ì•„ë©”ë¦¬ì¹´ë…¸ì™€ ê·¸ë¦°í‹° ìŠ¤ë¬´ë””ì˜ ìœ ì‚¬ë„
    2. the similarity evaluation tasks are not directly related to downstream tasks
        1. low testing corpus overlap
            1. sts corpusëŠ” ê·¸ë“¤ë§Œì˜ sourceê°€ ìˆë‹¤
        2. mismatch of tested properties
            1. stsì˜ í•™ìŠµëª©í‘œê°€ downstream taskì—ì„œëŠ” ê´€ì‹¬ì‚¬ê°€ ì•„ë‹ ìˆ˜ ìˆë‹¤.
                1. stsëŠ” ê°€ì¥ íš¨ê³¼ì ì¸ sts systemì„ ì°¾ê¸° ìœ„í•œ ê²ƒì´ì—ˆì§€, sentence embeddingì„ íš¨ê³¼ì ìœ¼ë¡œ ì°¾ê¸° ìœ„í•œ ê²ƒì´ ì•„ë‹ˆì—ˆë‹¤.
    3. the evaluation paradigm can be tricked with simple post-processing methods (overfitting)
        1. similarity metricì„ `cosine similarity` â†’ `l2`ë¡œ ë°”ê¾¸ëŠ” ê²ƒ ë§Œìœ¼ë¡œë„ ëª¨ë¸ë“¤ì˜ ìˆœìœ„ê°€ ë°”ë€ë‹¤.

            ![Untitled](/assets/img/posts/Just-Rank/Untitled%203.png)

        2. whitening tricks
            1. í”íˆ ì“°ëŠ” ê¸°ë²•: to obtain a more isotropic embedding space and can be summarized as a space **whitening process**
            2. whiteningì´ stsì—ëŠ” ë„ì›€ì´ ë˜ì§€ë§Œ downstream taskì—ëŠ” ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤
            3. whitening ê¸°ë²•ì´ ìœ ì‚¬ë„ê°€ ë‚®ì€ sampleë“¤ì˜ ì„±ëŠ¥ì„ ì˜¬ë¦¬ëŠ” ë°ë§Œ ë„ì›€ì´ ë˜ëŠ” ë“¯ í•˜ë‹¤.

                ![Untitled](/assets/img/posts/Just-Rank/Untitled%204.png)

{%endkatexmm%}